{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e57de57d-b4b5-41a7-8b97-ce05bb2e9b76",
   "metadata": {},
   "source": [
    "# centerflow\n",
    "\n",
    "**centerflow** is a Python module for modeling glacier dynamics along flowlines. It provides tools with five core functionalities:\n",
    "\n",
    "1. **id finder**\n",
    "    Sometimes we need multiple IDs for a single glacier. It's useful to be able to discern the RGIv6 ID from the RGIv7 ID, for example.\n",
    "\n",
    "2. **Mesh generation**  \n",
    "   Construct a 1D finite element mesh along an RGI-defined glacier centerline.\n",
    "\n",
    "\n",
    "3. **Mesh refinement***\n",
    "\n",
    "    Sometimes we need to limit the mesh to available data. \n",
    "\n",
    "4. **Data interpolation**  \n",
    "   Interpolate gridded geospatial datasets (e.g., surface elevation, velocity, surface mass balance) onto the centerline mesh.\n",
    "\n",
    "5. **Data re-interpolation**  \n",
    "   Extend interpolated functions onto a longer mesh, providing a buffer for terminus advance within mesh bounds. This is currently useless, but eventually maybe I'll get icepack2 working. \n",
    "\n",
    "6. **Bed inversion**  \n",
    "   Apply a forward-model-based bed inversion scheme following the approach of [van Pelt at al. (2013)](https://tc.copernicus.org/articles/7/987/2013/), using observed surface elevations to iteratively estimate basal topography.\n",
    "\n",
    "These tools are designed for efficient, reproducible glacier modeling along flowlines.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2319b02-b840-4848-96fc-6f3e57e3996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import firedrake\n",
    "import geopandas as gpd\n",
    "import icepack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyproj import Geod\n",
    "from pathlib import Path\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.crs import CRS\n",
    "from rasterio.io import MemoryFile\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from shapely.geometry import LineString\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3ae658-6aa1-4331-b13a-a2cf595e1e62",
   "metadata": {},
   "source": [
    "## rgi6_from_rgi7 etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd4d493-98bb-4200-b543-cbd5c19a7069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgi6_from_rgi7(**kwargs):\n",
    "    rgiid = kwargs[\"rgiid\"]\n",
    "    rgi6_path = kwargs[\"rgi6_path\"]\n",
    "    rgi7_path = kwargs[\"rgi7_path\"]\n",
    "\n",
    "    # Load RGI7 outlines and get the matching geometry\n",
    "    gdf7 = gpd.read_file(rgi7_path)\n",
    "    outline = gdf7[gdf7['rgi_id'].str.contains(rgiid, regex=False)].geometry.values[0]\n",
    "\n",
    "    # Load RGI6 outlines and normalize ID column\n",
    "    gdf6 = gpd.read_file(rgi6_path)\n",
    "    if \"RGIId\" in gdf6.columns:\n",
    "        gdf6 = gdf6[[\"RGIId\", \"geometry\"]].rename(columns={\"RGIId\": \"rgiid_6\"})\n",
    "    elif \"rgi_id\" in gdf6.columns:\n",
    "        gdf6 = gdf6[[\"rgi_id\", \"geometry\"]].rename(columns={\"rgi_id\": \"rgiid_6\"})\n",
    "    else:\n",
    "        raise ValueError(f\"No RGI ID column found in {rgi6_path}\")\n",
    "\n",
    "    # Project to metric CRS for area calculation\n",
    "    target_crs = \"EPSG:32646\"\n",
    "    gdf6 = gdf6.to_crs(target_crs)\n",
    "    outline_gdf = gpd.GeoDataFrame(geometry=[outline], crs=gdf7.crs).to_crs(target_crs)\n",
    "\n",
    "    # Intersect and pick the largest overlap\n",
    "    inter = gpd.overlay(gdf6, outline_gdf, how=\"intersection\")\n",
    "    if inter.empty:\n",
    "        return None\n",
    "    inter[\"overlap_area_m2\"] = inter.geometry.area\n",
    "    best_match = inter.sort_values(\"overlap_area_m2\", ascending=False).iloc[0][\"rgiid_6\"]\n",
    "\n",
    "    # Keep only the \"15.xxxxx\" part\n",
    "    return best_match.split(\"-\", 1)[-1]\n",
    "\n",
    "\n",
    "def rgi7_from_rgi6(**kwargs):\n",
    "    rgiid = kwargs[\"rgiid\"]\n",
    "    rgi6_path = kwargs[\"rgi6_path\"]\n",
    "    rgi7_path = kwargs[\"rgi7_path\"]\n",
    "\n",
    "    # Load RGI6 outlines and get the matching geometry\n",
    "    gdf6 = gpd.read_file(rgi6_path)\n",
    "    if \"RGIId\" in gdf6.columns:\n",
    "        gdf6 = gdf6[[\"RGIId\", \"geometry\"]].rename(columns={\"RGIId\": \"rgiid_6\"})\n",
    "    elif \"rgi_id\" in gdf6.columns:\n",
    "        gdf6 = gdf6[[\"rgi_id\", \"geometry\"]].rename(columns={\"rgi_id\": \"rgiid_6\"})\n",
    "    else:\n",
    "        raise ValueError(f\"No RGI ID column found in {rgi6_path}\")\n",
    "    outline = gdf6[gdf6['rgiid_6'].str.contains(rgiid, regex=False)].geometry.values[0]\n",
    "\n",
    "    # Load RGI7 outlines\n",
    "    gdf7 = gpd.read_file(rgi7_path)[[\"rgi_id\", \"geometry\"]]\n",
    "\n",
    "    # Project to metric CRS for area calculation\n",
    "    target_crs = \"EPSG:32646\"\n",
    "    gdf7 = gdf7.to_crs(target_crs)\n",
    "    outline_gdf = gpd.GeoDataFrame(geometry=[outline], crs=gdf6.crs).to_crs(target_crs)\n",
    "\n",
    "    # Intersect and pick the largest overlap\n",
    "    inter = gpd.overlay(gdf7, outline_gdf, how=\"intersection\")\n",
    "    if inter.empty:\n",
    "        return None\n",
    "    inter[\"overlap_area_m2\"] = inter.geometry.area\n",
    "    best_match = inter.sort_values(\"overlap_area_m2\", ascending=False).iloc[0][\"rgi_id\"]\n",
    "\n",
    "    # Keep only the \"15-xxxxx\" part\n",
    "    return \"-\".join(best_match.split(\"-\")[3:])\n",
    "\n",
    "def latlon_from_rgi7(**kwargs):\n",
    "    rgiid_7   = kwargs[\"rgiid\"]\n",
    "    rgi7_path = kwargs[\"rgi7_path\"]\n",
    "\n",
    "    # Load and filter\n",
    "    gdf7 = gpd.read_file(rgi7_path)\n",
    "    match = gdf7[gdf7['rgi_id'].str.contains(rgiid_7, regex=False)]\n",
    "    if match.empty:\n",
    "        raise ValueError(f\"RGI7 ID '{rgiid_7}' not found in {rgi7_path}\")\n",
    "\n",
    "    # Reproject to a metric CRS for centroid calculation\n",
    "    metric_crs = \"EPSG:32646\"  # Bhutan region\n",
    "    centroid_metric = match.to_crs(metric_crs).geometry.centroid.iloc[0]\n",
    "\n",
    "    # Convert centroid back to geographic CRS\n",
    "    centroid_geo = gpd.GeoSeries([centroid_metric], crs=metric_crs).to_crs(\"EPSG:4326\").iloc[0]\n",
    "    lat, lon = centroid_geo.y, centroid_geo.x\n",
    "\n",
    "    # Build tile string\n",
    "    lat_prefix = \"N\" if lat >= 0 else \"S\"\n",
    "    lon_prefix = \"E\" if lon >= 0 else \"W\"\n",
    "    lat_deg = int(np.floor(np.abs(lat)))\n",
    "    lon_deg = int(np.floor(np.abs(lon)))\n",
    "    tile = f\"{lat_prefix}{lat_deg:02d}{lon_prefix}{lon_deg:03d}\"\n",
    "\n",
    "    return tile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ee8f8b-88b8-4c94-9ded-62d0cf3bdf74",
   "metadata": {},
   "source": [
    "## centerline_mesh\n",
    "\n",
    "This is set up for consistency with RGI 7.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6248783f-fb05-4f84-ac9b-8d51c6f2c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class IntervalMeshResult:\n",
    "    mesh: firedrake.IntervalMesh\n",
    "    x: np.ndarray\n",
    "    y: np.ndarray\n",
    "    X: np.ndarray\n",
    "    glacier_length: float\n",
    "    mesh_length: float\n",
    "    centerline: object\n",
    "    centerline_extended: object\n",
    "    outline: object\n",
    "\n",
    "def centerline_mesh(**kwargs):\n",
    "    rgiid = kwargs.get('rgiid', '15-09534') #default to Luggye glacier (Bhutan) if no RGI ID is supplied\n",
    "    centerline_path = kwargs['centerline_path']\n",
    "    outline_path = kwargs['outline_path']\n",
    "    extra_length = kwargs.get('extra_length', 0) #extra length = L means we mesh L extra meters beyond the terminus. Currently not useful\n",
    "    n_cells = kwargs['n_cells']\n",
    "\n",
    "    outlines = gpd.read_file(outline_path)\n",
    "    centerlines = gpd.read_file(centerline_path)\n",
    "    outline = outlines[outlines['rgi_id'].str.contains(rgiid)].geometry.values[0]\n",
    "    flowlines = centerlines[centerlines.intersects(outline)] #may contain severeral smaller tributary flowlines\n",
    "    centerline = flowlines.loc[flowlines.to_crs('EPSG:32646').length.idxmax(), 'geometry'] #so grab the longest one\n",
    "\n",
    "    geod = Geod(ellps = 'WGS84')\n",
    "    x, y = centerline.xy\n",
    "    distances = [0] + [geod.inv(x[i], y[i], x[i+1], y[i+1])[2] for i in range(len(x) - 1)]\n",
    "    glacier_length = np.sum(distances)\n",
    "    length = glacier_length + extra_length #for the purposes of constructing the interval mesh\n",
    "\n",
    "    if extra_length > 0: #we'll make sure we include the appropriate amount of extra length, if necessary\n",
    "        azimuth, _, _ = geod.inv(x[-2], y[-2], x[-1], y[-1])\n",
    "        x_new, y_new, _ = geod.fwd(x[-1], y[-1], azimuth, extra_length)\n",
    "        x = np.append(x, x_new)\n",
    "        y = np.append(y, y_new)\n",
    "        distances.append(extra_length)\n",
    "    centerline_extended = LineString(np.column_stack((x, y)))\n",
    "\n",
    "        \n",
    "\n",
    "    mesh = firedrake.IntervalMesh(n_cells, length)\n",
    "    X = mesh.coordinates.dat.data_ro.flatten() #x coords in array form, useful for plotting values along the line\n",
    "\n",
    "    return IntervalMeshResult(\n",
    "        mesh = mesh,\n",
    "        x = np.array(x),\n",
    "        y = np.array(y),\n",
    "        X = X,\n",
    "        glacier_length = glacier_length,\n",
    "        mesh_length = float(length),\n",
    "        centerline = centerline,\n",
    "        centerline_extended = centerline_extended,\n",
    "        outline = outline,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a64a3e-88bd-49d9-ac70-513f664d32d6",
   "metadata": {},
   "source": [
    "## crop_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2c4139-589f-453c-928c-cf3fef8bf52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cut(line, d):\n",
    "    if d <= 0.0:\n",
    "        return [None, LineString(line.coords)]\n",
    "    if d >= line.length:\n",
    "        return [LineString(line.coords), None]\n",
    "    coords, acc = list(line.coords), 0.0\n",
    "    for i in range(len(coords) - 1):\n",
    "        p0, p1 = coords[i], coords[i + 1]\n",
    "        seg = LineString([p0, p1])\n",
    "        L = seg.length\n",
    "        if acc + L >= d:\n",
    "            t = (d - acc) / L\n",
    "            x = p0[0] + t * (p1[0] - p0[0])\n",
    "            y = p0[1] + t * (p1[1] - p0[1])\n",
    "            pt = (x, y)\n",
    "            return [LineString(coords[:i + 1] + [pt]), LineString([pt] + coords[i + 1:])]\n",
    "        acc += L\n",
    "    return [LineString(coords), None]\n",
    "\n",
    "def _segment(line, d0, d1):\n",
    "    left, mid = _cut(line, d0)\n",
    "    mid, right = _cut(mid, d1 - d0)\n",
    "    return mid\n",
    "\n",
    "def crop_mesh(**kwargs):\n",
    "    mesh = kwargs['mesh']\n",
    "    data_path = kwargs['data_path']\n",
    "\n",
    "    with rasterio.open(data_path) as src:\n",
    "        r_crs = src.crs\n",
    "        nodata = src.nodata\n",
    "\n",
    "    cl_m = gpd.GeoSeries([mesh.centerline_extended], crs = 'EPSG:4326').to_crs(r_crs).iloc[0]\n",
    "\n",
    "    target_samples = 400\n",
    "    step = max(cl_m.length / target_samples, 25.0)\n",
    "    dists = np.arange(0.0, cl_m.length + step, step, dtype = float)\n",
    "    pts = [cl_m.interpolate(float(d)) for d in dists]\n",
    "\n",
    "    with rasterio.open(data_path) as src:\n",
    "        vals = np.array([v[0] for v in src.sample([(p.x, p.y) for p in pts])])\n",
    "\n",
    "    valid = ~np.isnan(vals) if nodata is None else (vals != nodata)\n",
    "    if not valid.any():\n",
    "        raise ValueError('No valid data found along centerline for the provided raster.')\n",
    "\n",
    "    first = int(np.argmax(valid))\n",
    "    last = int(len(valid) - 1 - np.argmax(valid[::-1]))\n",
    "    d0, d1 = float(dists[first]), float(dists[last])\n",
    "\n",
    "    cl_m_cropped = _segment(cl_m, d0, d1)\n",
    "    cl_ll = gpd.GeoSeries([cl_m_cropped], crs = r_crs).to_crs('EPSG:4326').iloc[0]\n",
    "\n",
    "    geod = Geod(ellps = 'WGS84')\n",
    "    xs, ys = np.asarray(cl_ll.xy[0]), np.asarray(cl_ll.xy[1])\n",
    "    segs = [geod.inv(xs[i], ys[i], xs[i + 1], ys[i + 1])[2] for i in range(len(xs) - 1)]\n",
    "    new_len = float(np.sum(segs))\n",
    "\n",
    "    n_cells = mesh.mesh.num_cells()\n",
    "    new_mesh = firedrake.IntervalMesh(n_cells, new_len)\n",
    "    X = new_mesh.coordinates.dat.data_ro.flatten()\n",
    "\n",
    "    return type(mesh)(\n",
    "        mesh = new_mesh,\n",
    "        x = xs,\n",
    "        y = ys,\n",
    "        X = X,\n",
    "        glacier_length = new_len,\n",
    "        mesh_length = new_len,\n",
    "        centerline = cl_ll,\n",
    "        centerline_extended = cl_ll,\n",
    "        outline = mesh.outline,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbeb080-5ad1-4ffb-930b-46dc97ec7519",
   "metadata": {},
   "source": [
    "## map_to_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e77aa40-38c8-4d73-b6cc-9a434b98c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InterpolateResult:\n",
    "    data: firedrake.Function\n",
    "    last_nonzero_value: float\n",
    "\n",
    "def map_to_mesh(**kwargs):\n",
    "    mesh = kwargs['mesh']\n",
    "    data_path = kwargs['data_path']\n",
    "    extension = Path(data_path).suffix\n",
    "    dimension = kwargs.get('dimension', 1)\n",
    "    element = kwargs.get('element', 'CG')\n",
    "    ice_free_value = kwargs.get('ice_free_value', None)\n",
    "    key_value = kwargs.get('key_value', 'n/a')\n",
    "    data_value = kwargs.get('data_value', 'n/a')\n",
    "    key_dataset = kwargs.get('key_dataset', 'n/a')\n",
    "    projection = kwargs.get('projection', 'EPSG:4326')\n",
    "\n",
    "    if extension == '.tif':\n",
    "        x, y = mesh.x, mesh.y\n",
    "\n",
    "        with rasterio.open(data_path) as src:\n",
    "            src_crs = src.crs\n",
    "            target_crs = CRS.from_string(projection)\n",
    "\n",
    "            if src_crs != target_crs:\n",
    "                print(f'Reprojecting {data_path} from {src_crs} to {target_crs}')\n",
    "                transform, width, height = calculate_default_transform(\n",
    "                    src_crs, target_crs, src.width, src.height, *src.bounds)\n",
    "\n",
    "                meta = src.meta.copy()\n",
    "                meta.update({\n",
    "                    'crs': target_crs,\n",
    "                    'transform': transform,\n",
    "                    'width': width,\n",
    "                    'height': height\n",
    "                })\n",
    "\n",
    "                with MemoryFile() as memfile:\n",
    "                    with memfile.open(**meta) as dst:\n",
    "                        for i in range(1, src.count + 1):\n",
    "                            reproject(\n",
    "                                source = rasterio.band(src, i),\n",
    "                                destination = rasterio.band(dst, i),\n",
    "                                src_transform = src.transform,\n",
    "                                src_crs = src_crs,\n",
    "                                dst_transform = transform,\n",
    "                                dst_crs = target_crs,\n",
    "                                resampling = Resampling.bilinear\n",
    "                            )\n",
    "                    with memfile.open() as reproj:\n",
    "                        values = np.array(list(reproj.sample(zip(x, y)))).flatten()\n",
    "            else:\n",
    "                values = np.array(list(src.sample(zip(x, y)))).flatten()\n",
    "\n",
    "        distances = np.insert(np.cumsum([Geod(ellps = 'WGS84').inv(x[i], y[i], x[i+1], y[i+1])[2] for i in range(len(x) - 1)]), 0, 0)\n",
    "        vertex_coords = mesh.mesh.coordinates.dat.data_ro.flatten()\n",
    "        interp_vals = interp1d(distances, values, bounds_error = False, fill_value = 'extrapolate')(vertex_coords)\n",
    "        \n",
    "        if ice_free_value is not None:\n",
    "            interp_vals[vertex_coords > mesh.glacier_length] = ice_free_value\n",
    "\n",
    "        V = firedrake.FunctionSpace(mesh.mesh, element, dimension)\n",
    "        data_function = firedrake.Function(V)\n",
    "        data_function.dat.data[:] = interp_vals\n",
    "        last_nonzero = next((v for v in reversed(interp_vals) if v != 0), 0)\n",
    "\n",
    "        return InterpolateResult(data = data_function, last_nonzero_value = float(last_nonzero))\n",
    "\n",
    "    elif extension == '.csv':\n",
    "        data = pd.read_csv(data_path)\n",
    "        interp_func = interp1d(data[key_value], data[data_value], bounds_error = False, fill_value = 'extrapolate')\n",
    "\n",
    "        vertex_keys = key_dataset.dat.data_ro\n",
    "        data_on_mesh = interp_func(vertex_keys)\n",
    "\n",
    "        V = firedrake.FunctionSpace(mesh.mesh, element, dimension)\n",
    "        data_function = firedrake.Function(V)\n",
    "        data_function.dat.data[:] = data_on_mesh\n",
    "\n",
    "        return InterpolateResult(data = data_function, last_nonzero_value = np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da7eaaf-cd57-4f90-9b80-6cd5fdddc9cb",
   "metadata": {},
   "source": [
    "## extend_to_mesh\n",
    "\n",
    "This projects functions (which have already been defined on a short mesh) to a mesh which may extend beyond the glacier front. Unsure yet if this will be useful. It will depend on if/how icepack2 is incorporated to permit the modeling of zero-thickness domains.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52918d2-c94f-46c8-928a-cb549f5de3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_to_mesh(**kwargs):\n",
    "    source_function = kwargs['source_function']\n",
    "    source_mesh = kwargs['source_mesh']\n",
    "    target_mesh = kwargs['target_mesh']\n",
    "    ice_free_value = kwargs.get('ice_free_value', None)\n",
    "\n",
    "    # Step 1: Get distances along source mesh from vertex coordinates\n",
    "    coords_src = source_mesh.mesh.coordinates.dat.data_ro[:]\n",
    "    distances_src = np.insert(np.cumsum([\n",
    "        np.linalg.norm(coords_src[i + 1] - coords_src[i])\n",
    "        for i in range(len(coords_src) - 1)\n",
    "    ]), 0, 0)\n",
    "\n",
    "    values_src = source_function.dat.data_ro[:]\n",
    "    assert len(distances_src) == len(values_src), \"Mismatch in source distance and value lengths\"\n",
    "\n",
    "    # Step 2: Get distances along target mesh\n",
    "    coords_tgt = target_mesh.mesh.coordinates.dat.data_ro[:]\n",
    "    distances_tgt = np.insert(np.cumsum([\n",
    "        np.linalg.norm(coords_tgt[i + 1] - coords_tgt[i])\n",
    "        for i in range(len(coords_tgt) - 1)\n",
    "    ]), 0, 0)\n",
    "\n",
    "    # Step 3: Interpolate and apply cutoff\n",
    "    interp_func = interp1d(distances_src, values_src, bounds_error=False, fill_value='extrapolate')\n",
    "    values_tgt = interp_func(distances_tgt)\n",
    "\n",
    "    if ice_free_value is not None:\n",
    "        values_tgt[distances_tgt > source_mesh.glacier_length] = ice_free_value\n",
    "\n",
    "    # Step 4: Create new function on target mesh\n",
    "    V_new = firedrake.FunctionSpace(target_mesh.mesh, source_function.function_space().ufl_element())\n",
    "    f_new = firedrake.Function(V_new)\n",
    "    f_new.dat.data[:] = values_tgt\n",
    "\n",
    "    last_nonzero = next((v for v in reversed(values_tgt) if v != 0), 0)\n",
    "\n",
    "    return InterpolateResult(data=f_new, last_nonzero_value=float(last_nonzero))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e456fd0-7277-43b0-8118-6f9cdab0ba64",
   "metadata": {},
   "source": [
    "## smooth_function\n",
    "\n",
    "Sometimes this is useful I guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d601db4-63e0-46f0-bf99-8856150ce567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_function(**kwargs):\n",
    "    f = kwargs['function']\n",
    "    mesh = kwargs['mesh']\n",
    "    sigma = kwargs.get('sigma', None)\n",
    "    window_meters = kwargs.get('window', None)\n",
    "\n",
    "    f_data = f.dat.data_ro.copy()\n",
    "\n",
    "    # Compute dx (spacing in meters) from base mesh\n",
    "    coords = mesh.mesh.coordinates.dat.data_ro.flatten()\n",
    "    dx = np.mean(np.diff(coords))\n",
    "\n",
    "    if sigma is None:\n",
    "        if window_meters is None:\n",
    "            raise ValueError('Must specify either \"sigma\" or \"window_meters\"')\n",
    "        sigma = window_meters / dx / 2.0  # Approximate: 2σ ≈ full width at half maximum\n",
    "\n",
    "    smoothed_data = gaussian_filter1d(f_data, sigma = sigma)\n",
    "\n",
    "    f_smoothed = f.copy(deepcopy = True)\n",
    "    f_smoothed.dat.data[:] = smoothed_data\n",
    "\n",
    "    return f_smoothed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c464ff-e7d4-49a0-86c2-16a1b243a163",
   "metadata": {},
   "source": [
    "## solve_bed\n",
    "\n",
    "This process requires two surface elevation observations, separated by a known amount of time, with known SMB between image dates. \n",
    "\n",
    "The initial glacier configuration sets the first surface elevation above a guess bed elevation. We then forward model until the date of the second observation, and compare the resulting model surface with the final reference surface. The bed is adjusted according to the misfit, and the process begins again, with the updated bed guess in place of the initial guess. This continues for a specified number of iterations. \n",
    "\n",
    "If only one surface observation is available, then the solver will operate under the assumption that the glacier is in steady state, so that the second reference surface is identical to the first. This assumption can be made explicitly (setting ```surface``` and ```surface_2``` to the same function) or implicitly (defining only ```surface```). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45d6c84-6cde-44e3-b6a8-150e2df73d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InversionResult:\n",
    "    bed: firedrake.Function\n",
    "    misfits: list\n",
    "    bed_evolution: list\n",
    "    surface_evolution: list\n",
    "    velocity_evolution: list\n",
    "    thickness_evolution: list\n",
    "    s_ref: firedrake.Function\n",
    "    surface: firedrake.Function\n",
    "    velocity: firedrake.Function\n",
    "\n",
    "def solve_bed(**kwargs):\n",
    "    K = kwargs['K']\n",
    "    num_iterations = kwargs['num_iterations'] #number of iterations for the bed inversion\n",
    "    s_init = kwargs['surface'] #initial surface (first observation)\n",
    "    s_ref = kwargs.get('surface_2', s_init) #final surface (second observation) \n",
    "    H_guess = kwargs['thickness_guess'] #initial thickness guess\n",
    "    u_guess = kwargs['velocity'] #velocity boundary condition                    \n",
    "    a = kwargs['accumulation'] #a list of one specific SMB value/function for every modeled timestep, OR a single function for all steps\n",
    "    model = kwargs['model']\n",
    "    solver = kwargs['solver']\n",
    "    mesh = kwargs['mesh']\n",
    "    A = kwargs['fluidity']\n",
    "\n",
    "    try: num_years = s_ref.year - s_init.year #extract the time diff from the DEMs, if applicable\n",
    "    except: num_years = kwargs['model_time'] #otherwise, need to choose how long to model for \n",
    "\n",
    "    try: Δt = round(list(a)[1] - list(a)[0], 10) #extract Δt from the SMB list, if applicable\n",
    "    except: Δt = kwargs['timestep'] #otherwise, it needs to be specified\n",
    "        \n",
    "    Q = s_ref.function_space()\n",
    "\n",
    "    bed_guess = firedrake.Function(Q).project(s_init - H_guess)\n",
    "\n",
    "    misfits = []\n",
    "    bed_evolution = [np.array(bed_guess.dat.data_ro.copy())]\n",
    "    surface_evolution = []\n",
    "    velocity_evolution = []\n",
    "    thickness_evolution = []\n",
    "\n",
    "    num_timesteps = int(num_years / Δt)\n",
    "\n",
    "    bed_correction = firedrake.Function(Q)\n",
    "    surface_misfit = firedrake.Function(Q)\n",
    "\n",
    "    for iteration in trange(num_iterations):\n",
    "        bed_mod = bed_guess.copy(deepcopy = True)\n",
    "        H_mod = firedrake.Function(Q).project(s_init - bed_mod)\n",
    "        H_0 = H_mod.copy(deepcopy = True)\n",
    "        u_mod = u_guess.copy(deepcopy = True)\n",
    "        s_mod = s_init.copy(deepcopy = True)\n",
    "    \n",
    "        for step in range(num_timesteps):\n",
    "            \n",
    "            try: accumulation = a[s_init.year + step*Δt] #if SMB is a dictionary with date keys\n",
    "            except: accumulation = a #otherwise\n",
    "\n",
    "            try:\n",
    "                u_mod = solver.diagnostic_solve(\n",
    "                    velocity = u_mod, thickness = H_mod, surface = s_mod, fluidity = A\n",
    "                )\n",
    "                H_mod = solver.prognostic_solve(\n",
    "                    Δt, thickness = H_mod, velocity = u_mod,\n",
    "                    thickness_inflow = H_0, accumulation = accumulation\n",
    "                )\n",
    "                s_mod.project(bed_mod + H_mod)\n",
    "            \n",
    "            except:\n",
    "                print(f'Bed solver failed after {iteration} iterations')\n",
    "\n",
    "                return InversionResult(\n",
    "                    bed = bed_guess,\n",
    "                    misfits = misfits,\n",
    "                    bed_evolution = bed_evolution,\n",
    "                    surface_evolution = surface_evolution,\n",
    "                    velocity_evolution = velocity_evolution,\n",
    "                    thickness_evolution = thickness_evolution,\n",
    "                    s_ref = s_ref\n",
    "                )\n",
    "\n",
    "        surface_misfit.project(s_mod - s_ref)\n",
    "        bed_correction.project(-K*surface_misfit)\n",
    "        bed_guess.project(bed_mod + bed_correction)\n",
    "\n",
    "        misfits.append(float(firedrake.assemble(surface_misfit*firedrake.dx)/mesh.glacier_length))\n",
    "        # misfits.append(np.linalg.norm(np.array(surface_misfit.dat.data_ro.copy())))\n",
    "        bed_evolution.append(np.array(bed_guess.dat.data_ro.copy()))\n",
    "        surface_evolution.append(np.array(s_mod.dat.data_ro.copy()))\n",
    "        velocity_evolution.append(np.array(u_mod.dat.data_ro.copy()))\n",
    "        thickness_evolution.append(H_mod.dat.data_ro.copy())\n",
    "\n",
    "    return InversionResult(\n",
    "        bed = bed_guess,\n",
    "        misfits = misfits,\n",
    "        bed_evolution = bed_evolution,\n",
    "        surface_evolution = surface_evolution,\n",
    "        velocity_evolution = velocity_evolution,\n",
    "        thickness_evolution = thickness_evolution,\n",
    "        s_ref = s_ref,\n",
    "        surface = s_mod,\n",
    "        velocity = u_mod,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a268e3ac-a930-43d6-b20b-0356438213fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbconvert --to script centerflow.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dev2025)",
   "language": "python",
   "name": "dev2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
